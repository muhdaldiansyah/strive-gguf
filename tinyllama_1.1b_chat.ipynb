{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ TinyLlama 1.1B Chat\n",
    "\n",
    "**Model:** TinyLlama 1.1B Chat v1.0  \n",
    "**Size:** ~650MB (Q4_K_M quantized)  \n",
    "**Best for:** Fast inference, low memory usage, general chat\n",
    "\n",
    "---\n",
    "\n",
    "## How to use this notebook:\n",
    "1. Run **Step 1** to install and load the model (takes 2-3 minutes first time)\n",
    "2. Run **Step 2** to start chatting with the model\n",
    "3. Modify the prompt in Step 2 and run again for different responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Setup & Load Model\n",
    "\n",
    "This cell will:\n",
    "- Install required packages\n",
    "- Download the model (cached after first run)\n",
    "- Load the model into memory\n",
    "\n",
    "**Just run this cell and wait for \"‚úÖ Ready to chat!\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"üì• Installing packages...\")\n",
    "!pip install -q --no-cache-dir llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "# Import libraries\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MODEL_REPO = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
    "MODEL_FILE = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "os.environ[\"LLAMA_LOG_LEVEL\"] = \"ERROR\"\n",
    "os.environ[\"GGML_LOG_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "# Download model\n",
    "print(f\"‚¨áÔ∏è Downloading {MODEL_FILE}...\")\n",
    "model_path = hf_hub_download(repo_id=MODEL_REPO, filename=MODEL_FILE)\n",
    "print(f\"‚úì Model downloaded\")\n",
    "\n",
    "# Load model\n",
    "print(\"üîÑ Loading model into memory...\")\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,      # Use GPU acceleration\n",
    "    n_ctx=4096,           # Context window\n",
    "    verbose=False\n",
    ")\n",
    "print(\"‚úÖ Ready to chat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Step 2: Chat with the Model\n",
    "\n",
    "**Instructions:**\n",
    "- Edit the `user_prompt` below with your question\n",
    "- Run this cell to get a response\n",
    "- Run it multiple times with different prompts!\n",
    "\n",
    "**Tips:**\n",
    "- Increase `max_tokens` for longer responses (128-512)\n",
    "- Increase `temperature` (0.1-0.9) for more creative responses\n",
    "- Lower `temperature` (0.1-0.3) for more focused responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëá EDIT YOUR PROMPT HERE\n",
    "user_prompt = \"Explain what machine learning is in simple terms\"\n",
    "\n",
    "# Configuration (optional - adjust as needed)\n",
    "temperature = 0.7    # 0.0 = focused, 1.0 = creative\n",
    "max_tokens = 256     # Maximum response length\n",
    "\n",
    "# Generate response\n",
    "print(f\"üßë User: {user_prompt}\\n\")\n",
    "print(\"ü§ñ Assistant: \", end=\"\")\n",
    "\n",
    "response = llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Multi-turn Conversation (Optional)\n",
    "\n",
    "For conversations with context, use this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a conversation with context\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Python is a high-level programming language.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are its main advantages?\"}\n",
    "]\n",
    "\n",
    "response = llm.create_chat_completion(\n",
    "    messages=conversation,\n",
    "    temperature=0.7,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Assistant:\", response['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
